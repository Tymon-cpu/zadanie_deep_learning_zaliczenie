{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3lkFT_YyvHs"
      },
      "source": [
        "**Data Loader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNE3vaITxQFC",
        "outputId": "0a31562b-5e69-4fc3-98d1-9f06b3b963d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dataset załadowany: 586 zdjęć.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os, sys, torch, numpy as np, glob\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PATH = f\"/content/drive/MyDrive/Colab Notebooks/dane\"\n",
        "DATA_ROOT = os.path.join(PATH, \"dataset\")\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Mapowanie\n",
        "MAPPING = {24: 1, 26: 2, 33: 3, 27: 4, 28: 5}\n",
        "\n",
        "class CityscapesDataset(Dataset):\n",
        "    def __init__(self, root, split='train'):\n",
        "        self.files = sorted(glob.glob(os.path.join(root, split, 'images', '**', '*.png'), recursive=True))\n",
        "        self.transform = transforms.Compose([transforms.Resize((256, 512)), transforms.ToTensor()])\n",
        "\n",
        "    def __len__(self): return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.files[idx]\n",
        "        img = self.transform(Image.open(path).convert(\"RGB\"))\n",
        "\n",
        "        mask_path = path.replace('images', 'annotations').replace('_leftImg8bit.png', '_gtFine_labelIds.png')\n",
        "        if not os.path.exists(mask_path): mask_path = path.replace('images', 'annotations')\n",
        "\n",
        "        mask_pil = Image.open(mask_path).resize((512, 256), resample=Image.NEAREST)\n",
        "        mask_np = np.array(mask_pil)\n",
        "\n",
        "        target = np.zeros_like(mask_np)\n",
        "        for k, v in MAPPING.items(): target[mask_np == k] = v\n",
        "        return img, torch.from_numpy(target).long()\n",
        "\n",
        "# Ilośc danych\n",
        "ds = CityscapesDataset(DATA_ROOT)\n",
        "print(f\"Dataset załadowany: {len(ds)} zdjęć.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXp1kYiazpKv"
      },
      "source": [
        "**Ważone mIoU, metryki oraz model z dwiema głowicami**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpZjPeyYx_LI"
      },
      "outputs": [],
      "source": [
        "#Ważone mIoU\n",
        "class IncrementalIoU:\n",
        "    def __init__(self): self.mat = np.zeros((6, 6))\n",
        "    def update(self, p, t):\n",
        "        m = (t > 0) & (t < 6)\n",
        "        if m.sum() > 0: self.mat += np.bincount(6*t[m].flatten()+p[m].flatten(), minlength=36).reshape(6,6)\n",
        "    def compute(self):\n",
        "        d = np.diag(self.mat)\n",
        "        iou = d / (self.mat.sum(1) + self.mat.sum(0) - d + 1e-7)\n",
        "        return 0.4 * iou[1] + 0.15 * np.sum(iou[2:6])\n",
        "\n",
        "# Dice Loss\n",
        "def dice_loss(p, t):\n",
        "    probs = torch.softmax(p, 1)[:, 1:]\n",
        "    t_1h = torch.nn.functional.one_hot(t, 6).permute(0,3,1,2).float()[:, 1:]\n",
        "    return 1 - (2*(probs*t_1h).sum())/(probs.sum()+t_1h.sum()+1e-7)\n",
        "\n",
        "# Dual Head Model\n",
        "class DualHeadModel(nn.Module):\n",
        "    def __init__(self, arch=\"deeplabv3\"):\n",
        "        super().__init__()\n",
        "        if arch == \"deeplabv3\":\n",
        "            base = models.segmentation.deeplabv3_resnet50(weights=\"DEFAULT\")\n",
        "            in_ch = 256\n",
        "        else:\n",
        "            base = models.segmentation.fcn_resnet50(weights=\"DEFAULT\")\n",
        "            in_ch = 512\n",
        "\n",
        "        self.backbone = base.backbone\n",
        "        self.classifier = nn.Sequential(*list(base.classifier.children())[:-1])\n",
        "        self.head_ped = nn.Conv2d(in_ch, 2, 1)\n",
        "        self.head_others = nn.Conv2d(in_ch, 6, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.classifier(self.backbone(x)['out'])\n",
        "        p = F.interpolate(self.head_ped(feat), size=x.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        o = F.interpolate(self.head_others(feat), size=x.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        return {'ped': p, 'others': o}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaQvbDMO0E2F"
      },
      "source": [
        "**Trening i Porównanie**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8K56SV5yJYc",
        "outputId": "d0ac0596-1db8-41c3-dc35-f004778cd511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Porównywanie modeli po 32 kroków każdy\n",
            "Trenowanie: DeepLabV3 + CrossEntropy...\n",
            "Wynik mIoU: 0.4691\n",
            "Trenowanie: DeepLabV3 + DiceLoss...\n",
            "Wynik mIoU: 0.1296\n",
            "Trenowanie: FCN + CrossEntropy...\n",
            "Wynik mIoU: 0.4558\n",
            "Podsumowanie modeli:\n",
            "       Model          Loss      mIoU\n",
            "0  DeepLabV3  CrossEntropy  0.469139\n",
            "1  DeepLabV3      DiceLoss  0.129631\n",
            "2        FCN  CrossEntropy  0.455821\n"
          ]
        }
      ],
      "source": [
        "print(\"Porównywanie modeli po 32 kroków każdy\")\n",
        "\n",
        "configs = [\n",
        "    (\"DeepLabV3\", \"CrossEntropy\"),\n",
        "    (\"DeepLabV3\", \"DiceLoss\"),\n",
        "    (\"FCN\", \"CrossEntropy\")\n",
        "]\n",
        "\n",
        "results = []\n",
        "ds = CityscapesDataset(DATA_ROOT, split='train')\n",
        "\n",
        "for arch, loss_name in configs:\n",
        "    print(f\"Trenowanie: {arch} + {loss_name}...\")\n",
        "    model = DualHeadModel(arch.lower()).to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "    if loss_name == \"CrossEntropy\": criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    else: criterion = dice_loss\n",
        "\n",
        "    model.train()\n",
        "    loader = DataLoader(ds, batch_size=4, shuffle=True)\n",
        "    steps = 0\n",
        "\n",
        "    for img, target in loader:\n",
        "        img, target = img.to(DEVICE), target.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(img)\n",
        "\n",
        "        if loss_name == \"CrossEntropy\":\n",
        "            l_ped, l_oth = criterion(out['ped'], (target==1).long()), criterion(out['others'], target)\n",
        "        else:\n",
        "            l_ped, l_oth = dice_loss(out['ped'], target), dice_loss(out['others'], target)\n",
        "\n",
        "        (0.7 * l_ped + 0.3 * l_oth).backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        steps += 1\n",
        "        if steps >= 32: break\n",
        "\n",
        "    # Ewaluacja\n",
        "    model.eval()\n",
        "    metric = IncrementalIoU()\n",
        "    with torch.no_grad():\n",
        "        for i, (img, target) in enumerate(loader):\n",
        "            if i >= 15: break\n",
        "            img, target = img.to(DEVICE), target.to(DEVICE)\n",
        "            metric.update(model(img)['others'].argmax(1).cpu().numpy(), target.cpu().numpy())\n",
        "\n",
        "    score = metric.compute()\n",
        "    print(f\"Wynik mIoU: {score:.4f}\")\n",
        "    results.append({\"Model\": arch, \"Loss\": loss_name, \"mIoU\": score})\n",
        "\n",
        "print(\"Podsumowanie modeli:\")\n",
        "print(pd.DataFrame(results))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq6qsZFLyXJI"
      },
      "source": [
        "# Wnioski\n",
        "Najlepszą wydajność mIoU (0,469) osiągnął model DeepLabV3 przy zastosowaniu funkcji straty CrossEntropy. Architektura ta okazała się skuteczniejsza od modelu FCN, choć przy tej samej funkcji straty różnica między nimi była niewielka. Zastosowanie DiceLoss zamiast CrossEntropy drastycznie obniżyło jakość segmentacji, co czyni tę konfigurację najmniej efektywną w zestawieniu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIJEoY6U0MHh"
      },
      "source": [
        "**Trenowanie najlepszego modelu**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezz0-2oRyamm",
        "outputId": "81a16975-7b38-4b12-8aee-ef91d9f98f9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rozpoczynam trening (DeepLabV3 + CE) na 3 epok...\n",
            " Epoka 1/3 zakończona. Liczę mIoU...\n",
            "Epoka 1: Loss=nan | mIoU=0.6377 | Czas: 112.0 min\n",
            " Zapisano nowy najlepszy model!\n",
            " Epoka 2/3 zakończona. Liczę mIoU...\n",
            "Epoka 2: Loss=0.0596 | mIoU=0.6914 | Czas: 112.4 min\n",
            " Zapisano nowy najlepszy model!\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch.optim as optim\n",
        "\n",
        "NUM_EPOCHS = 3\n",
        "BEST_MIOU = 0.0\n",
        "\n",
        "print(f\"Rozpoczynam trening (DeepLabV3 + CE) na {NUM_EPOCHS} epok...\")\n",
        "\n",
        "model = DualHeadModel(\"deeplabv3\").to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "ds_train = CityscapesDataset(DATA_ROOT, split='train')\n",
        "loader = DataLoader(ds_train, batch_size=4, shuffle=True)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for i, (img, target) in enumerate(loader):\n",
        "        img, target = img.to(DEVICE), target.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out = model(img)\n",
        "\n",
        "        l_ped = criterion(out['ped'], (target==1).long())\n",
        "        l_oth = criterion(out['others'], target)\n",
        "\n",
        "        loss = 0.7 * l_ped + 0.3 * l_oth\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    metric = IncrementalIoU()\n",
        "    print(f\" Epoka {epoch+1}/{NUM_EPOCHS} zakończona. Liczę mIoU...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (img, target) in enumerate(loader):\n",
        "            if i >= 50: break\n",
        "            img, target = img.to(DEVICE), target.to(DEVICE)\n",
        "            preds = model(img)['others'].argmax(1).cpu().numpy()\n",
        "            metric.update(preds, target.cpu().numpy())\n",
        "\n",
        "    current_miou = metric.compute()\n",
        "    duration = (time.time() - start_time) / 60\n",
        "\n",
        "    print(f\"Epoka {epoch+1}: Loss={epoch_loss/len(loader):.4f} | mIoU={current_miou:.4f} | Czas: {duration:.1f} min\")\n",
        "\n",
        "    if current_miou > BEST_MIOU:\n",
        "        BEST_MIOU = current_miou\n",
        "        torch.save(model.state_dict(), \"best_model_cityscapes.pth\")\n",
        "        print(\" Zapisano nowy najlepszy model!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"WYNIK KOŃCOWY: mIoU = {BEST_MIOU:.4f}\")\n",
        "print(\"=\"*50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}