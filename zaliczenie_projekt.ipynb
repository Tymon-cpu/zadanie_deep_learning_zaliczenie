{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3lkFT_YyvHs"
   },
   "source": [
    "**Data Loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kNE3vaITxQFC"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os, sys, torch, numpy as np, glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "PATH = f\"/content/drive/MyDrive/Colab Notebooks/dane\"\n",
    "DATA_ROOT = os.path.join(PATH, \"dataset\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Mapowanie\n",
    "MAPPING = {24: 1, 26: 2, 33: 3, 27: 4, 28: 5}\n",
    "\n",
    "class CityscapesDataset(Dataset):\n",
    "    def __init__(self, root, split='train'):\n",
    "        self.files = sorted(glob.glob(os.path.join(root, split, 'images', '**', '*.png'), recursive=True))\n",
    "        self.transform = transforms.Compose([transforms.Resize((256, 512)), transforms.ToTensor()])\n",
    "\n",
    "    def __len__(self): return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.files[idx]\n",
    "        img = self.transform(Image.open(path).convert(\"RGB\"))\n",
    "\n",
    "        mask_path = path.replace('images', 'annotations').replace('_leftImg8bit.png', '_gtFine_labelIds.png')\n",
    "        if not os.path.exists(mask_path): mask_path = path.replace('images', 'annotations')\n",
    "\n",
    "        mask_pil = Image.open(mask_path).resize((512, 256), resample=Image.NEAREST)\n",
    "        mask_np = np.array(mask_pil)\n",
    "\n",
    "        target = np.zeros_like(mask_np)\n",
    "        for k, v in MAPPING.items(): target[mask_np == k] = v\n",
    "        return img, torch.from_numpy(target).long()\n",
    "\n",
    "# Ilośc danych\n",
    "ds = CityscapesDataset(DATA_ROOT)\n",
    "print(f\"Dataset załadowany: {len(ds)} zdjęć.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXp1kYiazpKv"
   },
   "source": [
    "**Ważone mIoU, metryki oraz model z dwiema głowicami**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KpZjPeyYx_LI"
   },
   "outputs": [],
   "source": [
    "#Ważone mIoU\n",
    "class IncrementalIoU:\n",
    "    def __init__(self): self.mat = np.zeros((6, 6))\n",
    "    def update(self, p, t):\n",
    "        m = (t > 0) & (t < 6)\n",
    "        if m.sum() > 0: self.mat += np.bincount(6*t[m].flatten()+p[m].flatten(), minlength=36).reshape(6,6)\n",
    "    def compute(self):\n",
    "        d = np.diag(self.mat)\n",
    "        iou = d / (self.mat.sum(1) + self.mat.sum(0) - d + 1e-7)\n",
    "        return 0.4 * iou[1] + 0.15 * np.sum(iou[2:6])\n",
    "\n",
    "# Dice Loss\n",
    "def dice_loss(p, t):\n",
    "    probs = torch.softmax(p, 1)[:, 1:]\n",
    "    t_1h = torch.nn.functional.one_hot(t, 6).permute(0,3,1,2).float()[:, 1:]\n",
    "    return 1 - (2*(probs*t_1h).sum())/(probs.sum()+t_1h.sum()+1e-7)\n",
    "\n",
    "# Dual Head Model\n",
    "class DualHeadModel(nn.Module):\n",
    "    def __init__(self, arch=\"deeplabv3\"):\n",
    "        super().__init__()\n",
    "        if arch == \"deeplabv3\":\n",
    "            base = models.segmentation.deeplabv3_resnet50(weights=\"DEFAULT\")\n",
    "            in_ch = 256\n",
    "        else:\n",
    "            base = models.segmentation.fcn_resnet50(weights=\"DEFAULT\")\n",
    "            in_ch = 512\n",
    "\n",
    "        self.backbone = base.backbone\n",
    "        self.classifier = nn.Sequential(*list(base.classifier.children())[:-1])\n",
    "        self.head_ped = nn.Conv2d(in_ch, 2, 1)\n",
    "        self.head_others = nn.Conv2d(in_ch, 6, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.classifier(self.backbone(x)['out'])\n",
    "        p = F.interpolate(self.head_ped(feat), size=x.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        o = F.interpolate(self.head_others(feat), size=x.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        return {'ped': p, 'others': o}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaQvbDMO0E2F"
   },
   "source": [
    "**Trening i Porównanie**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8K56SV5yJYc"
   },
   "outputs": [],
   "source": [
    "print(\"Porównywanie modeli po 32 kroków każdy\")\n",
    "\n",
    "configs = [\n",
    "    (\"DeepLabV3\", \"CrossEntropy\"),\n",
    "    (\"DeepLabV3\", \"DiceLoss\"),\n",
    "    (\"FCN\", \"CrossEntropy\")\n",
    "]\n",
    "\n",
    "results = []\n",
    "ds = CityscapesDataset(DATA_ROOT, split='train')\n",
    "\n",
    "for arch, loss_name in configs:\n",
    "    print(f\"Trenowanie: {arch} + {loss_name}...\")\n",
    "    model = DualHeadModel(arch.lower()).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    if loss_name == \"CrossEntropy\": criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    else: criterion = dice_loss\n",
    "\n",
    "    model.train()\n",
    "    loader = DataLoader(ds, batch_size=4, shuffle=True)\n",
    "    steps = 0\n",
    "\n",
    "    for img, target in loader:\n",
    "        img, target = img.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(img)\n",
    "\n",
    "        if loss_name == \"CrossEntropy\":\n",
    "            l_ped, l_oth = criterion(out['ped'], (target==1).long()), criterion(out['others'], target)\n",
    "        else:\n",
    "            l_ped, l_oth = dice_loss(out['ped'], target), dice_loss(out['others'], target)\n",
    "\n",
    "        (0.7 * l_ped + 0.3 * l_oth).backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        steps += 1\n",
    "        if steps >= 32: break\n",
    "\n",
    "    # Ewaluacja\n",
    "    model.eval()\n",
    "    metric = IncrementalIoU()\n",
    "    with torch.no_grad():\n",
    "        for i, (img, target) in enumerate(loader):\n",
    "            if i >= 15: break\n",
    "            img, target = img.to(DEVICE), target.to(DEVICE)\n",
    "            metric.update(model(img)['others'].argmax(1).cpu().numpy(), target.cpu().numpy())\n",
    "\n",
    "    score = metric.compute()\n",
    "    print(f\"Wynik mIoU: {score:.4f}\")\n",
    "    results.append({\"Model\": arch, \"Loss\": loss_name, \"mIoU\": score})\n",
    "\n",
    "print(\"Podsumowanie modeli:\")\n",
    "print(pd.DataFrame(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yq6qsZFLyXJI"
   },
   "source": [
    "# Wnioski\n",
    "\n",
    "\n",
    "1. Architektura: DeepLabV3 vs FCN\n",
    "DeepLabV3 zdecydowanie wygrywa. Dzięki modułowi ASPP sieć lepiej ogarnia obiekty o różnych rozmiarach (np. pieszych blisko i daleko). FCN jest na to za prosty i przy bardziej złożonych scenach po prostu wymięka.\n",
    "\n",
    "2. Funkcja straty: CE vs Dice\n",
    "Mimo że Dice Loss miał pomagać na niezbalansowane klasy, to CrossEntropy dowiozła lepszy wynik. Była dużo stabilniejsza i łatwiejsza do wytrenowania. Dice Loss okazał się zbyt „kapryśny” i przy tak krótkim treningu trudno było go sensownie ustawić.\n",
    "\n",
    "3. Podejście Dual Head\n",
    "Wrzucenie dwóch zadań do jednego modelu to strzał w dziesiątkę pod kątem wydajności:\n",
    "\n",
    "- Plusy: Model jest lekki i szybki, bo „mieli” obraz tylko raz. Idealne do systemów real-time.  \n",
    "- Minusy: Trzeba ręcznie pilnować wag strat, żeby nauka wykrywania ludzi nie „zjadła” segmentacji reszty otoczenia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIJEoY6U0MHh"
   },
   "source": [
    "**Trenowanie najlepszego modelu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ezz0-2oRyamm"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "BEST_MIOU = 0.0\n",
    "\n",
    "print(f\"Rozpoczynam trening (DeepLabV3 + CE) na {NUM_EPOCHS} epok...\")\n",
    "\n",
    "model = DualHeadModel(\"deeplabv3\").to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "ds_train = CityscapesDataset(DATA_ROOT, split='train')\n",
    "loader = DataLoader(ds_train, batch_size=4, shuffle=True)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for i, (img, target) in enumerate(loader):\n",
    "        img, target = img.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(img)\n",
    "\n",
    "        l_ped = criterion(out['ped'], (target==1).long())\n",
    "        l_oth = criterion(out['others'], target)\n",
    "\n",
    "        loss = 0.7 * l_ped + 0.3 * l_oth\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    metric = IncrementalIoU()\n",
    "    print(f\" Epoka {epoch+1}/{NUM_EPOCHS} zakończona. Liczę mIoU...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (img, target) in enumerate(loader):\n",
    "            if i >= 50: break\n",
    "            img, target = img.to(DEVICE), target.to(DEVICE)\n",
    "            preds = model(img)['others'].argmax(1).cpu().numpy()\n",
    "            metric.update(preds, target.cpu().numpy())\n",
    "\n",
    "    current_miou = metric.compute()\n",
    "    duration = (time.time() - start_time) / 60\n",
    "\n",
    "    print(f\"Epoka {epoch+1}: Loss={epoch_loss/len(loader):.4f} | mIoU={current_miou:.4f} | Czas: {duration:.1f} min\")\n",
    "\n",
    "    if current_miou > BEST_MIOU:\n",
    "        BEST_MIOU = current_miou\n",
    "        torch.save(model.state_dict(), \"best_model_cityscapes.pth\")\n",
    "        print(\" Zapisano nowy najlepszy model!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"WYNIK KOŃCOWY: mIoU = {BEST_MIOU:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPDMXYibV0FLJS7iw0PzB0O",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
